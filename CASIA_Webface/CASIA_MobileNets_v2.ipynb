{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from matplotlib.image import imread\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import Parameter\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path\n",
    "\n",
    "trainset_path = '/dataset/casia_webface_imgs_112_112'\n",
    "testset_path = '/dataset/LFW/lfw'\n",
    "\n",
    "# training dataset preprocessing\n",
    "\n",
    "trainset_preprocess = transforms.Compose([\n",
    "    transforms.Resize((112,112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# test dataset preprocessing\n",
    "\n",
    "testset_preprocess = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(trainset_path, transform=trainset_preprocess)\n",
    "#testset = ImageFolder(trainset_path, transform=testset_preprocess)\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [440623, 50000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10572\n",
      "490623\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "\n",
    "DATASET_CLASS = len(dataset.classes)\n",
    "DATASET_DATA = len(dataset)\n",
    "\n",
    "\n",
    "# same size of Celeb-500K-2R 98.2% Acc on LFW, VR@FAR=0 57.67% (ResNet20)\n",
    "print(DATASET_CLASS)\n",
    "print(DATASET_DATA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, expansion, stride):\n",
    "        super(Block, self).__init__()\n",
    "        self.stride = stride\n",
    "\n",
    "        planes = expansion * in_planes\n",
    "        # channel expansion\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        # depthwise convolution\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, groups=planes, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride == 1:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(out_planes),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu6(self.bn1(self.conv1(x)))\n",
    "        out = F.relu6(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out = out + self.shortcut(x) if self.stride==1 else out\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    cfg = [(1,  16, 1, 1),\n",
    "           (6,  24, 2, 1),  \n",
    "           (6,  32, 3, 2),\n",
    "           (6,  64, 4, 2),\n",
    "           (6,  96, 3, 1),\n",
    "           (6, 160, 3, 2),\n",
    "           (6, 224, 1, 1)]\n",
    "\n",
    "    def __init__(self, num_classes=10572):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.layers = self._make_layers(in_planes=32)\n",
    "        self.conv2 = nn.Conv2d(224, 256, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        self.linear = nn.Linear(256, num_classes)\n",
    "\n",
    "    def _make_layers(self, in_planes):\n",
    "        layers = []\n",
    "        for expansion, out_planes, num_blocks, stride in self.cfg:\n",
    "            strides = [stride] + [1]*(num_blocks-1)\n",
    "            for stride in strides:\n",
    "                layers.append(Block(in_planes, out_planes, expansion, stride))\n",
    "                in_planes = out_planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layers(out)\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        \n",
    "        out = F.avg_pool2d(out, 14)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accuracy = [] \n",
    "test_losses = []\n",
    "test_accuracy = []\n",
    "\n",
    "def train(epoch):\n",
    "    print('\\n[ Train epoch: %d ]' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, prediction = outputs.max(1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += prediction.eq(labels).sum().item()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('\\nCurrent batch:', str(batch_idx))\n",
    "            print('Current train accuracy:', str(prediction.eq(labels).sum().item() / labels.size(0)))\n",
    "            print('Current train average loss:', loss.item() / labels.size(0))\n",
    "\n",
    "            train_losses.append(loss.item() / labels.size(0))\n",
    "            train_accuracy.append(prediction.eq(labels).sum().item() / labels.size(0))\n",
    "            \n",
    "    print('\\nTrain accuarcy:', correct / total)\n",
    "    print('Train average loss:', train_loss / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    print('\\n[ Test epoch: %d ]' % epoch)\n",
    "    net.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        total += labels.size(0)\n",
    "\n",
    "        outputs = net(images)\n",
    "        loss += criterion(outputs, labels).item()\n",
    "\n",
    "        _, prediction = outputs.max(1)\n",
    "        correct += prediction.eq(labels).sum().item()\n",
    "\n",
    "    print('\\nTest accuarcy:', correct / total)\n",
    "    print('Test average loss:', loss / total)\n",
    "    test_losses.append(loss / total)\n",
    "    test_accuracy.append(correct / total)\n",
    "\n",
    "    state = {\n",
    "        'net': net.state_dict()\n",
    "    }\n",
    "\n",
    "    file_name = 'CASIA_MobileNet_v2.pt'\n",
    "    if not os.path.isdir('checkpoint'):\n",
    "        os.mkdir('checkpoint')\n",
    "    torch.save(state, './checkpoint/' + file_name)\n",
    "    print('Model Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MobileNetV2()\n",
    "net = net.to(device)\n",
    "\n",
    "learning_rate = 0.1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = learning_rate\n",
    "    #if epoch >= 50:\n",
    "    if epoch >= 50:\n",
    "        lr /= 10\n",
    "    #if epoch >= 100:\n",
    "    if epoch >= 100:\n",
    "        lr /= 10\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.625036 M\n"
     ]
    }
   ],
   "source": [
    "print(count_parameters(net)/1000000,str('M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: update in /opt/conda/lib/python3.6/site-packages (0.0.1)\n",
      "Requirement already satisfied: style==1.1.0 in /opt/conda/lib/python3.6/site-packages (from update) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ptflops in /opt/conda/lib/python3.6/site-packages (0.6.6)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from ptflops) (1.8.0a0+17f8c32)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.6/site-packages (from torch->ptflops) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torch->ptflops) (1.19.2)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from torch->ptflops) (0.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ptflops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/sovrasov/flops-counter.pytorch.git\n",
      "  Cloning https://github.com/sovrasov/flops-counter.pytorch.git to /tmp/pip-req-build-4upm6a2u\n",
      "Requirement already satisfied, skipping upgrade: torch in /opt/conda/lib/python3.6/site-packages (from ptflops==0.6.6) (1.8.0a0+17f8c32)\n",
      "Requirement already satisfied, skipping upgrade: typing_extensions in /opt/conda/lib/python3.6/site-packages (from torch->ptflops==0.6.6) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/conda/lib/python3.6/site-packages (from torch->ptflops==0.6.6) (1.19.2)\n",
      "Requirement already satisfied, skipping upgrade: dataclasses in /opt/conda/lib/python3.6/site-packages (from torch->ptflops==0.6.6) (0.7)\n",
      "Building wheels for collected packages: ptflops\n",
      "  Building wheel for ptflops (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ptflops: filename=ptflops-0.6.6-py3-none-any.whl size=9719 sha256=70684615aad282970090633bd3608dbd1bbb350f9184604e34dab08b9cfcc937\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-jr02i11v/wheels/b7/08/f3/c24c594062d3fe3a282194cfd29bc4d05f7496f7e971b7645a\n",
      "Successfully built ptflops\n",
      "Installing collected packages: ptflops\n",
      "  Attempting uninstall: ptflops\n",
      "    Found existing installation: ptflops 0.6.6\n",
      "    Uninstalling ptflops-0.6.6:\n",
      "      Successfully uninstalled ptflops-0.6.6\n",
      "Successfully installed ptflops-0.6.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade git+https://github.com/sovrasov/flops-counter.pytorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module Block is treated as a zero-op.\n",
      "Warning: module MobileNetV2 is treated as a zero-op.\n",
      "MobileNetV2(\n",
      "  4.625 M, 100.000% Params, 1.122 GMac, 100.000% MACs, \n",
      "  (conv1): Conv2d(0.001 M, 0.019% Params, 0.011 GMac, 0.966% MACs, 3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(0.0 M, 0.001% Params, 0.001 GMac, 0.072% MACs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    1.849 M, 39.983% Params, 1.096 GMac, 97.709% MACs, \n",
      "    (0): Block(\n",
      "      0.003 M, 0.055% Params, 0.032 GMac, 2.845% MACs, \n",
      "      (conv1): Conv2d(0.001 M, 0.022% Params, 0.013 GMac, 1.145% MACs, 32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.0 M, 0.001% Params, 0.001 GMac, 0.072% MACs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.0 M, 0.006% Params, 0.004 GMac, 0.322% MACs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(0.0 M, 0.001% Params, 0.001 GMac, 0.072% MACs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.001 M, 0.011% Params, 0.006 GMac, 0.572% MACs, 32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.036% MACs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        0.001 M, 0.012% Params, 0.007 GMac, 0.626% MACs, \n",
      "        (0): Conv2d(0.001 M, 0.011% Params, 0.006 GMac, 0.572% MACs, 32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.036% MACs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.018% MACs, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      0.006 M, 0.120% Params, 0.07 GMac, 6.253% MACs, \n",
      "      (conv1): Conv2d(0.002 M, 0.033% Params, 0.019 GMac, 1.717% MACs, 16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.0 M, 0.004% Params, 0.002 GMac, 0.215% MACs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.001 M, 0.019% Params, 0.011 GMac, 0.966% MACs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
      "      (bn2): BatchNorm2d(0.0 M, 0.004% Params, 0.002 GMac, 0.215% MACs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.002 M, 0.050% Params, 0.029 GMac, 2.576% MACs, 96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.001% Params, 0.001 GMac, 0.054% MACs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        0.0 M, 0.009% Params, 0.006 GMac, 0.510% MACs, \n",
      "        (0): Conv2d(0.0 M, 0.008% Params, 0.005 GMac, 0.429% MACs, 16, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(0.0 M, 0.001% Params, 0.001 GMac, 0.054% MACs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.027% MACs, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      0.009 M, 0.204% Params, 0.119 GMac, 10.600% MACs, \n",
      "      (conv1): Conv2d(0.003 M, 0.075% Params, 0.043 GMac, 3.864% MACs, 24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.0 M, 0.006% Params, 0.004 GMac, 0.322% MACs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.001 M, 0.028% Params, 0.016 GMac, 1.449% MACs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "      (bn2): BatchNorm2d(0.0 M, 0.006% Params, 0.004 GMac, 0.322% MACs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.003 M, 0.075% Params, 0.043 GMac, 3.864% MACs, 144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.001% Params, 0.001 GMac, 0.054% MACs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        0.001 M, 0.013% Params, 0.008 GMac, 0.725% MACs, \n",
      "        (0): Conv2d(0.001 M, 0.012% Params, 0.007 GMac, 0.644% MACs, 24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(0.0 M, 0.001% Params, 0.001 GMac, 0.054% MACs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.027% MACs, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      0.01 M, 0.216% Params, 0.067 GMac, 5.935% MACs, \n",
      "      (conv1): Conv2d(0.003 M, 0.075% Params, 0.043 GMac, 3.864% MACs, 24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.0 M, 0.006% Params, 0.004 GMac, 0.322% MACs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.001 M, 0.028% Params, 0.004 GMac, 0.362% MACs, 144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "      (bn2): BatchNorm2d(0.0 M, 0.006% Params, 0.001 GMac, 0.081% MACs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.005 M, 0.100% Params, 0.014 GMac, 1.288% MACs, 144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.018% MACs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "    )\n",
      "    (4): Block(\n",
      "      0.016 M, 0.345% Params, 0.05 GMac, 4.464% MACs, \n",
      "      (conv1): Conv2d(0.006 M, 0.133% Params, 0.019 GMac, 1.717% MACs, 32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.0 M, 0.008% Params, 0.001 GMac, 0.107% MACs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.002 M, 0.037% Params, 0.005 GMac, 0.483% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "      (bn2): BatchNorm2d(0.0 M, 0.008% Params, 0.001 GMac, 0.107% MACs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.006 M, 0.133% Params, 0.019 GMac, 1.717% MACs, 192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.018% MACs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        0.001 M, 0.024% Params, 0.004 GMac, 0.313% MACs, \n",
      "        (0): Conv2d(0.001 M, 0.022% Params, 0.003 GMac, 0.286% MACs, 32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.018% MACs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.009% MACs, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      0.016 M, 0.345% Params, 0.05 GMac, 4.464% MACs, \n",
      "      (conv1): Conv2d(0.006 M, 0.133% Params, 0.019 GMac, 1.717% MACs, 32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.0 M, 0.008% Params, 0.001 GMac, 0.107% MACs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.002 M, 0.037% Params, 0.005 GMac, 0.483% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "      (bn2): BatchNorm2d(0.0 M, 0.008% Params, 0.001 GMac, 0.107% MACs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.006 M, 0.133% Params, 0.019 GMac, 1.717% MACs, 192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.018% MACs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        0.001 M, 0.024% Params, 0.004 GMac, 0.313% MACs, \n",
      "        (0): Conv2d(0.001 M, 0.022% Params, 0.003 GMac, 0.286% MACs, 32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.018% MACs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.009% MACs, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      0.021 M, 0.455% Params, 0.032 GMac, 2.840% MACs, \n",
      "      (conv1): Conv2d(0.006 M, 0.133% Params, 0.019 GMac, 1.717% MACs, 32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.0 M, 0.008% Params, 0.001 GMac, 0.107% MACs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.002 M, 0.037% Params, 0.001 GMac, 0.121% MACs, 192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "      (bn2): BatchNorm2d(0.0 M, 0.008% Params, 0.0 GMac, 0.027% MACs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.012 M, 0.266% Params, 0.01 GMac, 0.859% MACs, 192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.003% Params, 0.0 GMac, 0.009% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "    )\n",
      "    (7): Block(\n",
      "      0.058 M, 1.265% Params, 0.046 GMac, 4.092% MACs, \n",
      "      (conv1): Conv2d(0.025 M, 0.531% Params, 0.019 GMac, 1.717% MACs, 64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.017% Params, 0.001 GMac, 0.054% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.003 M, 0.075% Params, 0.003 GMac, 0.242% MACs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.017% Params, 0.001 GMac, 0.054% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.025 M, 0.531% Params, 0.019 GMac, 1.717% MACs, 384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.003% Params, 0.0 GMac, 0.009% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        0.004 M, 0.091% Params, 0.003 GMac, 0.300% MACs, \n",
      "        (0): Conv2d(0.004 M, 0.089% Params, 0.003 GMac, 0.286% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(0.0 M, 0.003% Params, 0.0 GMac, 0.009% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.004% MACs, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      0.058 M, 1.265% Params, 0.046 GMac, 4.092% MACs, \n",
      "      (conv1): Conv2d(0.025 M, 0.531% Params, 0.019 GMac, 1.717% MACs, 64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.017% Params, 0.001 GMac, 0.054% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.003 M, 0.075% Params, 0.003 GMac, 0.242% MACs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.017% Params, 0.001 GMac, 0.054% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.025 M, 0.531% Params, 0.019 GMac, 1.717% MACs, 384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.003% Params, 0.0 GMac, 0.009% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        0.004 M, 0.091% Params, 0.003 GMac, 0.300% MACs, \n",
      "        (0): Conv2d(0.004 M, 0.089% Params, 0.003 GMac, 0.286% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(0.0 M, 0.003% Params, 0.0 GMac, 0.009% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.004% MACs, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      0.058 M, 1.265% Params, 0.046 GMac, 4.092% MACs, \n",
      "      (conv1): Conv2d(0.025 M, 0.531% Params, 0.019 GMac, 1.717% MACs, 64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.017% Params, 0.001 GMac, 0.054% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.003 M, 0.075% Params, 0.003 GMac, 0.242% MACs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.017% Params, 0.001 GMac, 0.054% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.025 M, 0.531% Params, 0.019 GMac, 1.717% MACs, 384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.003% Params, 0.0 GMac, 0.009% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        0.004 M, 0.091% Params, 0.003 GMac, 0.300% MACs, \n",
      "        (0): Conv2d(0.004 M, 0.089% Params, 0.003 GMac, 0.286% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(0.0 M, 0.003% Params, 0.0 GMac, 0.009% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.004% MACs, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      0.073 M, 1.578% Params, 0.057 GMac, 5.105% MACs, \n",
      "      (conv1): Conv2d(0.025 M, 0.531% Params, 0.019 GMac, 1.717% MACs, 64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.017% Params, 0.001 GMac, 0.054% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.003 M, 0.075% Params, 0.003 GMac, 0.242% MACs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.017% Params, 0.001 GMac, 0.054% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.037 M, 0.797% Params, 0.029 GMac, 2.576% MACs, 384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.004% Params, 0.0 GMac, 0.013% MACs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        0.006 M, 0.137% Params, 0.005 GMac, 0.449% MACs, \n",
      "        (0): Conv2d(0.006 M, 0.133% Params, 0.005 GMac, 0.429% MACs, 64, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(0.0 M, 0.004% Params, 0.0 GMac, 0.013% MACs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.007% MACs, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      0.128 M, 2.761% Params, 0.1 GMac, 8.929% MACs, \n",
      "      (conv1): Conv2d(0.055 M, 1.196% Params, 0.043 GMac, 3.864% MACs, 96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.025% Params, 0.001 GMac, 0.081% MACs, 576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.005 M, 0.112% Params, 0.004 GMac, 0.362% MACs, 576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.025% Params, 0.001 GMac, 0.081% MACs, 576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.055 M, 1.196% Params, 0.043 GMac, 3.864% MACs, 576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.004% Params, 0.0 GMac, 0.013% MACs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        0.009 M, 0.203% Params, 0.007 GMac, 0.664% MACs, \n",
      "        (0): Conv2d(0.009 M, 0.199% Params, 0.007 GMac, 0.644% MACs, 96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(0.0 M, 0.004% Params, 0.0 GMac, 0.013% MACs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.007% MACs, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (12): Block(\n",
      "      0.128 M, 2.761% Params, 0.1 GMac, 8.929% MACs, \n",
      "      (conv1): Conv2d(0.055 M, 1.196% Params, 0.043 GMac, 3.864% MACs, 96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.025% Params, 0.001 GMac, 0.081% MACs, 576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.005 M, 0.112% Params, 0.004 GMac, 0.362% MACs, 576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.025% Params, 0.001 GMac, 0.081% MACs, 576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.055 M, 1.196% Params, 0.043 GMac, 3.864% MACs, 576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.004% Params, 0.0 GMac, 0.013% MACs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        0.009 M, 0.203% Params, 0.007 GMac, 0.664% MACs, \n",
      "        (0): Conv2d(0.009 M, 0.199% Params, 0.007 GMac, 0.644% MACs, 96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(0.0 M, 0.004% Params, 0.0 GMac, 0.013% MACs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.007% MACs, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (13): Block(\n",
      "      0.155 M, 3.357% Params, 0.064 GMac, 5.671% MACs, \n",
      "      (conv1): Conv2d(0.055 M, 1.196% Params, 0.043 GMac, 3.864% MACs, 96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.025% Params, 0.001 GMac, 0.081% MACs, 576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.005 M, 0.112% Params, 0.001 GMac, 0.091% MACs, 576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.025% Params, 0.0 GMac, 0.020% MACs, 576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.092 M, 1.993% Params, 0.018 GMac, 1.610% MACs, 576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.007% Params, 0.0 GMac, 0.006% MACs, 160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "    )\n",
      "    (14): Block(\n",
      "      0.346 M, 7.479% Params, 0.068 GMac, 6.046% MACs, \n",
      "      (conv1): Conv2d(0.154 M, 3.321% Params, 0.03 GMac, 2.684% MACs, 160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.002 M, 0.042% Params, 0.0 GMac, 0.034% MACs, 960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.009 M, 0.187% Params, 0.002 GMac, 0.151% MACs, 960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "      (bn2): BatchNorm2d(0.002 M, 0.042% Params, 0.0 GMac, 0.034% MACs, 960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.154 M, 3.321% Params, 0.03 GMac, 2.684% MACs, 960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.007% Params, 0.0 GMac, 0.006% MACs, 160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        0.026 M, 0.560% Params, 0.005 GMac, 0.456% MACs, \n",
      "        (0): Conv2d(0.026 M, 0.554% Params, 0.005 GMac, 0.447% MACs, 160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(0.0 M, 0.007% Params, 0.0 GMac, 0.006% MACs, 160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.003% MACs, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (15): Block(\n",
      "      0.346 M, 7.479% Params, 0.068 GMac, 6.046% MACs, \n",
      "      (conv1): Conv2d(0.154 M, 3.321% Params, 0.03 GMac, 2.684% MACs, 160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.002 M, 0.042% Params, 0.0 GMac, 0.034% MACs, 960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.009 M, 0.187% Params, 0.002 GMac, 0.151% MACs, 960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "      (bn2): BatchNorm2d(0.002 M, 0.042% Params, 0.0 GMac, 0.034% MACs, 960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.154 M, 3.321% Params, 0.03 GMac, 2.684% MACs, 960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.007% Params, 0.0 GMac, 0.006% MACs, 160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        0.026 M, 0.560% Params, 0.005 GMac, 0.456% MACs, \n",
      "        (0): Conv2d(0.026 M, 0.554% Params, 0.005 GMac, 0.447% MACs, 160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(0.0 M, 0.007% Params, 0.0 GMac, 0.006% MACs, 160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.003% MACs, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (16): Block(\n",
      "      0.418 M, 9.035% Params, 0.082 GMac, 7.304% MACs, \n",
      "      (conv1): Conv2d(0.154 M, 3.321% Params, 0.03 GMac, 2.684% MACs, 160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.002 M, 0.042% Params, 0.0 GMac, 0.034% MACs, 960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.009 M, 0.187% Params, 0.002 GMac, 0.151% MACs, 960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "      (bn2): BatchNorm2d(0.002 M, 0.042% Params, 0.0 GMac, 0.034% MACs, 960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.215 M, 4.649% Params, 0.042 GMac, 3.757% MACs, 960, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.010% Params, 0.0 GMac, 0.008% MACs, 224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        0.036 M, 0.785% Params, 0.007 GMac, 0.638% MACs, \n",
      "        (0): Conv2d(0.036 M, 0.775% Params, 0.007 GMac, 0.626% MACs, 160, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(0.0 M, 0.010% Params, 0.0 GMac, 0.008% MACs, 224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.004% MACs, inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv2): Conv2d(0.057 M, 1.240% Params, 0.011 GMac, 1.002% MACs, 224, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(0.001 M, 0.011% Params, 0.0 GMac, 0.009% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear): Linear(2.717 M, 58.746% Params, 0.003 GMac, 0.242% MACs, in_features=256, out_features=10572, bias=True)\n",
      ")\n",
      "Computational complexity:       1.12 GMac\n",
      "Number of parameters:           4.63 M  \n",
      "= \n"
     ]
    }
   ],
   "source": [
    "# Multiply-Accumulate\n",
    "\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "with torch.cuda.device(0):\n",
    "    macs, params = get_model_complexity_info(net, (3, 112, 112), as_strings=True,\n",
    "                                           print_per_layer_stat=True, verbose=True)\n",
    "    print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "    print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "    print('= ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ Train epoch: 0 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.290746808052063\n",
      "\n",
      "Current batch: 100\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.2830881178379059\n",
      "\n",
      "Current batch: 200\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.2856045961380005\n",
      "\n",
      "Current batch: 300\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.273034006357193\n",
      "\n",
      "Current batch: 400\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.2785714864730835\n",
      "\n",
      "Current batch: 500\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.2811509370803833\n",
      "\n",
      "Current batch: 600\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.27899518609046936\n",
      "\n",
      "Current batch: 700\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.27694523334503174\n",
      "\n",
      "Current batch: 800\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.26692748069763184\n",
      "\n",
      "Current batch: 900\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.28205645084381104\n",
      "\n",
      "Current batch: 1000\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.2784344255924225\n",
      "\n",
      "Current batch: 1100\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.25878971815109253\n",
      "\n",
      "Current batch: 1200\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.27088624238967896\n",
      "\n",
      "Current batch: 1300\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.27670973539352417\n",
      "\n",
      "Current batch: 1400\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.27405059337615967\n",
      "\n",
      "Current batch: 1500\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.2710060775279999\n",
      "\n",
      "Current batch: 1600\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.2797265648841858\n",
      "\n",
      "Current batch: 1700\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.2834339737892151\n",
      "\n",
      "Current batch: 1800\n",
      "Current train accuracy: 0.03125\n",
      "Current train average loss: 0.26705294847488403\n",
      "\n",
      "Current batch: 1900\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.25514963269233704\n",
      "\n",
      "Current batch: 2000\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.2677140235900879\n",
      "\n",
      "Current batch: 2100\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.27895691990852356\n",
      "\n",
      "Current batch: 2200\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.2658114433288574\n",
      "\n",
      "Current batch: 2300\n",
      "Current train accuracy: 0.03125\n",
      "Current train average loss: 0.26214879751205444\n",
      "\n",
      "Current batch: 2400\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.24844148755073547\n",
      "\n",
      "Current batch: 2500\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.2640438377857208\n",
      "\n",
      "Current batch: 2600\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.250132292509079\n",
      "\n",
      "Current batch: 2700\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.2642218768596649\n",
      "\n",
      "Current batch: 2800\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.24815988540649414\n",
      "\n",
      "Current batch: 2900\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.2756738066673279\n",
      "\n",
      "Current batch: 3000\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.2676158845424652\n",
      "\n",
      "Current batch: 3100\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.2528384029865265\n",
      "\n",
      "Current batch: 3200\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.2603422701358795\n",
      "\n",
      "Current batch: 3300\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.27349454164505005\n",
      "\n",
      "Current batch: 3400\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.26179710030555725\n",
      "\n",
      "Current batch: 3500\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.2562308609485626\n",
      "\n",
      "Current batch: 3600\n",
      "Current train accuracy: 0.03125\n",
      "Current train average loss: 0.26654380559921265\n",
      "\n",
      "Current batch: 3700\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.23727037012577057\n",
      "\n",
      "Current batch: 3800\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.27143269777297974\n",
      "\n",
      "Current batch: 3900\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.2579324245452881\n",
      "\n",
      "Current batch: 4000\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.2715268135070801\n",
      "\n",
      "Current batch: 4100\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.2696240544319153\n",
      "\n",
      "Current batch: 4200\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.2563706636428833\n",
      "\n",
      "Current batch: 4300\n",
      "Current train accuracy: 0.03125\n",
      "Current train average loss: 0.2437269687652588\n",
      "\n",
      "Current batch: 4400\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.25229474902153015\n",
      "\n",
      "Current batch: 4500\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.25167685747146606\n",
      "\n",
      "Current batch: 4600\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.26140740513801575\n",
      "\n",
      "Current batch: 4700\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.24898122251033783\n",
      "\n",
      "Current batch: 4800\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.24526488780975342\n",
      "\n",
      "Current batch: 4900\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.2401779592037201\n",
      "\n",
      "Current batch: 5000\n",
      "Current train accuracy: 0.03125\n",
      "Current train average loss: 0.25013113021850586\n",
      "\n",
      "Current batch: 5100\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.26957398653030396\n",
      "\n",
      "Current batch: 5200\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.24520142376422882\n",
      "\n",
      "Current batch: 5300\n",
      "Current train accuracy: 0.03125\n",
      "Current train average loss: 0.25152620673179626\n",
      "\n",
      "Current batch: 5400\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.24433636665344238\n",
      "\n",
      "Current batch: 5500\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.2645748257637024\n",
      "\n",
      "Current batch: 5600\n",
      "Current train accuracy: 0.03125\n",
      "Current train average loss: 0.2610557973384857\n",
      "\n",
      "Current batch: 5700\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.24085931479930878\n",
      "\n",
      "Current batch: 5800\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.24810896813869476\n",
      "\n",
      "Current batch: 5900\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.2678042948246002\n",
      "\n",
      "Current batch: 6000\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.25129327178001404\n",
      "\n",
      "Current batch: 6100\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.26365458965301514\n",
      "\n",
      "Current batch: 6200\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.2534623444080353\n",
      "\n",
      "Current batch: 6300\n",
      "Current train accuracy: 0.03125\n",
      "Current train average loss: 0.24419324100017548\n",
      "\n",
      "Current batch: 6400\n",
      "Current train accuracy: 0.03125\n",
      "Current train average loss: 0.23968830704689026\n",
      "\n",
      "Current batch: 6500\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.23916789889335632\n",
      "\n",
      "Current batch: 6600\n",
      "Current train accuracy: 0.0625\n",
      "Current train average loss: 0.2424313724040985\n",
      "\n",
      "Current batch: 6700\n",
      "Current train accuracy: 0.03125\n",
      "Current train average loss: 0.2293699085712433\n",
      "\n",
      "Current batch: 6800\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.2522351145744324\n",
      "\n",
      "Current batch: 6900\n",
      "Current train accuracy: 0.0\n",
      "Current train average loss: 0.24722278118133545\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(0, 100):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    print('\\nTime elapsed:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_l = pd.DataFrame(train_losses, columns = ['train_losses'])\n",
    "df_train_l.plot(color = \"#ff0000\")\n",
    "plt.plot(df_train_l, marker = '*', color = 'r')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_a = pd.DataFrame(train_accuracy, columns = ['train_accuracy'])\n",
    "df_train_a.plot(color = \"#ff1111\")\n",
    "plt.plot(df_train_a, marker = '.', color = 'b')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_l = pd.DataFrame(test_losses, columns = ['test_losses'])\n",
    "df_test_l.plot(color = \"#ff0000\")\n",
    "plt.plot(df_test_l, marker = '*', color = 'r')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_a = pd.DataFrame(test_accuracy, columns = ['test_accuracy'])\n",
    "df_test_a.plot(color = \"#ff0000\")\n",
    "plt.plot(df_test_a, marker = '*', color = 'r')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
