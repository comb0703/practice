{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from matplotlib.image import imread\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import Parameter\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path\n",
    "\n",
    "trainset_path = '/dataset/casia_webface_imgs_112_112'\n",
    "testset_path = '/dataset/LFW/lfw'\n",
    "\n",
    "# training dataset preprocessing\n",
    "\n",
    "trainset_preprocess = transforms.Compose([\n",
    "    transforms.Resize((112,112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# test dataset preprocessing\n",
    "\n",
    "testset_preprocess = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(trainset_path, transform=trainset_preprocess)\n",
    "#testset = ImageFolder(trainset_path, transform=testset_preprocess)\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [440623, 50000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10572\n",
      "490623\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "\n",
    "DATASET_CLASS = len(dataset.classes)\n",
    "DATASET_DATA = len(dataset)\n",
    "\n",
    "\n",
    "# same size of Celeb-500K-2R 98.2% Acc on LFW, VR@FAR=0 57.67% (ResNet20)\n",
    "print(DATASET_CLASS)\n",
    "print(DATASET_DATA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, expansion, stride):\n",
    "        super(Block, self).__init__()\n",
    "        self.stride = stride\n",
    "\n",
    "        planes = expansion * in_planes\n",
    "        # channel expansion\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        # depthwise convolution\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, groups=planes, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride == 1:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(out_planes),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu6(self.bn1(self.conv1(x)))\n",
    "        out = F.relu6(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out = out + self.shortcut(x) if self.stride==1 else out\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    cfg = [(1,  16, 1, 1),\n",
    "           (6,  32, 3, 2),\n",
    "           (6,  64, 3, 2),\n",
    "           (6, 128, 3, 2)]\n",
    "\n",
    "    def __init__(self, num_classes=10572):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.layers = self._make_layers(in_planes=32)\n",
    "        self.conv2 = nn.Conv2d(128, 256, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        self.linear = nn.Linear(256, num_classes)\n",
    "\n",
    "    def _make_layers(self, in_planes):\n",
    "        layers = []\n",
    "        for expansion, out_planes, num_blocks, stride in self.cfg:\n",
    "            strides = [stride] + [1]*(num_blocks-1)\n",
    "            for stride in strides:\n",
    "                layers.append(Block(in_planes, out_planes, expansion, stride))\n",
    "                in_planes = out_planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layers(out)\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        \n",
    "        out = F.avg_pool2d(out, 14)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accuracy = [] \n",
    "test_losses = []\n",
    "test_accuracy = []\n",
    "\n",
    "def train(epoch):\n",
    "    print('\\n[ Train epoch: %d ]' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, prediction = outputs.max(1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += prediction.eq(labels).sum().item()\n",
    "\n",
    "        if batch_idx % 1000 == 0:\n",
    "            print('\\nCurrent batch:', str(batch_idx))\n",
    "            print('Current train accuracy:', str(prediction.eq(labels).sum().item() / labels.size(0)))\n",
    "            print('Current train average loss:', loss.item() / labels.size(0))\n",
    "\n",
    "            train_losses.append(loss.item() / labels.size(0))\n",
    "            train_accuracy.append(prediction.eq(labels).sum().item() / labels.size(0))\n",
    "            \n",
    "    print('\\nTrain accuarcy:', correct / total)\n",
    "    print('Train average loss:', train_loss / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    print('\\n[ Test epoch: %d ]' % epoch)\n",
    "    net.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        total += labels.size(0)\n",
    "\n",
    "        outputs = net(images)\n",
    "        loss += criterion(outputs, labels).item()\n",
    "\n",
    "        _, prediction = outputs.max(1)\n",
    "        correct += prediction.eq(labels).sum().item()\n",
    "\n",
    "    print('\\nTest accuarcy:', correct / total)\n",
    "    print('Test average loss:', loss / total)\n",
    "    test_losses.append(loss / total)\n",
    "    test_accuracy.append(correct / total)\n",
    "\n",
    "    state = {\n",
    "        'net': net.state_dict()\n",
    "    }\n",
    "\n",
    "    file_name = 'CASIA_MobileNet_v2.pt'\n",
    "    if not os.path.isdir('checkpoint'):\n",
    "        os.mkdir('checkpoint')\n",
    "    torch.save(state, './checkpoint/' + file_name)\n",
    "    print('Model Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MobileNetV2()\n",
    "net = net.to(device)\n",
    "\n",
    "learning_rate = 0.1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = learning_rate\n",
    "    #if epoch >= 50:\n",
    "    if epoch >= 50:\n",
    "        lr /= 10\n",
    "    #if epoch >= 100:\n",
    "    if epoch >= 100:\n",
    "        lr /= 10\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.455532 M\n"
     ]
    }
   ],
   "source": [
    "print(count_parameters(net)/1000000,str('M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: update in /opt/conda/lib/python3.6/site-packages (0.0.1)\n",
      "Requirement already satisfied: style==1.1.0 in /opt/conda/lib/python3.6/site-packages (from update) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ptflops in /opt/conda/lib/python3.6/site-packages (0.6.6)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from ptflops) (1.8.0a0+17f8c32)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.6/site-packages (from torch->ptflops) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torch->ptflops) (1.19.2)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from torch->ptflops) (0.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ptflops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/sovrasov/flops-counter.pytorch.git\n",
      "  Cloning https://github.com/sovrasov/flops-counter.pytorch.git to /tmp/pip-req-build-kf24stud\n",
      "Requirement already satisfied, skipping upgrade: torch in /opt/conda/lib/python3.6/site-packages (from ptflops==0.6.6) (1.8.0a0+17f8c32)\n",
      "Requirement already satisfied, skipping upgrade: typing_extensions in /opt/conda/lib/python3.6/site-packages (from torch->ptflops==0.6.6) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/conda/lib/python3.6/site-packages (from torch->ptflops==0.6.6) (1.19.2)\n",
      "Requirement already satisfied, skipping upgrade: dataclasses in /opt/conda/lib/python3.6/site-packages (from torch->ptflops==0.6.6) (0.7)\n",
      "Building wheels for collected packages: ptflops\n",
      "  Building wheel for ptflops (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ptflops: filename=ptflops-0.6.6-py3-none-any.whl size=9719 sha256=48d24653386c90b7c539404cd3ddefb75b37bef60806f8be5937ddd0264efbf4\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-n33r_b40/wheels/b7/08/f3/c24c594062d3fe3a282194cfd29bc4d05f7496f7e971b7645a\n",
      "Successfully built ptflops\n",
      "Installing collected packages: ptflops\n",
      "  Attempting uninstall: ptflops\n",
      "    Found existing installation: ptflops 0.6.6\n",
      "    Uninstalling ptflops-0.6.6:\n",
      "      Successfully uninstalled ptflops-0.6.6\n",
      "Successfully installed ptflops-0.6.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade git+https://github.com/sovrasov/flops-counter.pytorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module Block is treated as a zero-op.\n",
      "Warning: module MobileNetV2 is treated as a zero-op.\n",
      "MobileNetV2(\n",
      "  3.456 M, 100.000% Params, 0.429 GMac, 100.000% MACs, \n",
      "  (conv1): Conv2d(0.001 M, 0.025% Params, 0.011 GMac, 2.523% MACs, 3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(0.0 M, 0.002% Params, 0.001 GMac, 0.187% MACs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    0.704 M, 20.382% Params, 0.409 GMac, 95.138% MACs, \n",
      "    (0): Block(\n",
      "      0.003 M, 0.073% Params, 0.032 GMac, 7.430% MACs, \n",
      "      (conv1): Conv2d(0.001 M, 0.030% Params, 0.013 GMac, 2.991% MACs, 32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.0 M, 0.002% Params, 0.001 GMac, 0.187% MACs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.0 M, 0.008% Params, 0.004 GMac, 0.841% MACs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(0.0 M, 0.002% Params, 0.001 GMac, 0.187% MACs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.001 M, 0.015% Params, 0.006 GMac, 1.495% MACs, 32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.093% MACs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        0.001 M, 0.016% Params, 0.007 GMac, 1.636% MACs, \n",
      "        (0): Conv2d(0.001 M, 0.015% Params, 0.006 GMac, 1.495% MACs, 32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.093% MACs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.047% MACs, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      0.006 M, 0.171% Params, 0.035 GMac, 8.108% MACs, \n",
      "      (conv1): Conv2d(0.002 M, 0.044% Params, 0.019 GMac, 4.486% MACs, 16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.0 M, 0.006% Params, 0.002 GMac, 0.561% MACs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.001 M, 0.025% Params, 0.003 GMac, 0.631% MACs, 96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "      (bn2): BatchNorm2d(0.0 M, 0.006% Params, 0.001 GMac, 0.140% MACs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.003 M, 0.089% Params, 0.01 GMac, 2.243% MACs, 96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.002% Params, 0.0 GMac, 0.047% MACs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "    )\n",
      "    (2): Block(\n",
      "      0.016 M, 0.461% Params, 0.05 GMac, 11.659% MACs, \n",
      "      (conv1): Conv2d(0.006 M, 0.178% Params, 0.019 GMac, 4.486% MACs, 32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.0 M, 0.011% Params, 0.001 GMac, 0.280% MACs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.002 M, 0.050% Params, 0.005 GMac, 1.262% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "      (bn2): BatchNorm2d(0.0 M, 0.011% Params, 0.001 GMac, 0.280% MACs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.006 M, 0.178% Params, 0.019 GMac, 4.486% MACs, 192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.002% Params, 0.0 GMac, 0.047% MACs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        0.001 M, 0.031% Params, 0.004 GMac, 0.818% MACs, \n",
      "        (0): Conv2d(0.001 M, 0.030% Params, 0.003 GMac, 0.748% MACs, 32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(0.0 M, 0.002% Params, 0.0 GMac, 0.047% MACs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.023% MACs, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      0.016 M, 0.461% Params, 0.05 GMac, 11.659% MACs, \n",
      "      (conv1): Conv2d(0.006 M, 0.178% Params, 0.019 GMac, 4.486% MACs, 32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.0 M, 0.011% Params, 0.001 GMac, 0.280% MACs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.002 M, 0.050% Params, 0.005 GMac, 1.262% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "      (bn2): BatchNorm2d(0.0 M, 0.011% Params, 0.001 GMac, 0.280% MACs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.006 M, 0.178% Params, 0.019 GMac, 4.486% MACs, 192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.002% Params, 0.0 GMac, 0.047% MACs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        0.001 M, 0.031% Params, 0.004 GMac, 0.818% MACs, \n",
      "        (0): Conv2d(0.001 M, 0.030% Params, 0.003 GMac, 0.748% MACs, 32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(0.0 M, 0.002% Params, 0.0 GMac, 0.047% MACs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.023% MACs, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      0.021 M, 0.609% Params, 0.032 GMac, 7.419% MACs, \n",
      "      (conv1): Conv2d(0.006 M, 0.178% Params, 0.019 GMac, 4.486% MACs, 32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.0 M, 0.011% Params, 0.001 GMac, 0.280% MACs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.002 M, 0.050% Params, 0.001 GMac, 0.315% MACs, 192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "      (bn2): BatchNorm2d(0.0 M, 0.011% Params, 0.0 GMac, 0.070% MACs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.012 M, 0.356% Params, 0.01 GMac, 2.243% MACs, 192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.004% Params, 0.0 GMac, 0.023% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "    )\n",
      "    (5): Block(\n",
      "      0.058 M, 1.693% Params, 0.046 GMac, 10.690% MACs, \n",
      "      (conv1): Conv2d(0.025 M, 0.711% Params, 0.019 GMac, 4.486% MACs, 64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.022% Params, 0.001 GMac, 0.140% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.003 M, 0.100% Params, 0.003 GMac, 0.631% MACs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.022% Params, 0.001 GMac, 0.140% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.025 M, 0.711% Params, 0.019 GMac, 4.486% MACs, 384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.004% Params, 0.0 GMac, 0.023% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        0.004 M, 0.122% Params, 0.003 GMac, 0.783% MACs, \n",
      "        (0): Conv2d(0.004 M, 0.119% Params, 0.003 GMac, 0.748% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(0.0 M, 0.004% Params, 0.0 GMac, 0.023% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.012% MACs, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      0.058 M, 1.693% Params, 0.046 GMac, 10.690% MACs, \n",
      "      (conv1): Conv2d(0.025 M, 0.711% Params, 0.019 GMac, 4.486% MACs, 64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.022% Params, 0.001 GMac, 0.140% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.003 M, 0.100% Params, 0.003 GMac, 0.631% MACs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.022% Params, 0.001 GMac, 0.140% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.025 M, 0.711% Params, 0.019 GMac, 4.486% MACs, 384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.004% Params, 0.0 GMac, 0.023% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        0.004 M, 0.122% Params, 0.003 GMac, 0.783% MACs, \n",
      "        (0): Conv2d(0.004 M, 0.119% Params, 0.003 GMac, 0.748% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(0.0 M, 0.004% Params, 0.0 GMac, 0.023% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.012% MACs, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      0.079 M, 2.285% Params, 0.03 GMac, 7.074% MACs, \n",
      "      (conv1): Conv2d(0.025 M, 0.711% Params, 0.019 GMac, 4.486% MACs, 64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.022% Params, 0.001 GMac, 0.140% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.003 M, 0.100% Params, 0.001 GMac, 0.158% MACs, 384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.022% Params, 0.0 GMac, 0.035% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.049 M, 1.422% Params, 0.01 GMac, 2.243% MACs, 384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.007% Params, 0.0 GMac, 0.012% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "    )\n",
      "    (8): Block(\n",
      "      0.223 M, 6.468% Params, 0.044 GMac, 10.205% MACs, \n",
      "      (conv1): Conv2d(0.098 M, 2.845% Params, 0.019 GMac, 4.486% MACs, 128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.002 M, 0.044% Params, 0.0 GMac, 0.070% MACs, 768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.007 M, 0.200% Params, 0.001 GMac, 0.315% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
      "      (bn2): BatchNorm2d(0.002 M, 0.044% Params, 0.0 GMac, 0.070% MACs, 768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.098 M, 2.845% Params, 0.019 GMac, 4.486% MACs, 768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.007% Params, 0.0 GMac, 0.012% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        0.017 M, 0.482% Params, 0.003 GMac, 0.765% MACs, \n",
      "        (0): Conv2d(0.016 M, 0.474% Params, 0.003 GMac, 0.748% MACs, 128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(0.0 M, 0.007% Params, 0.0 GMac, 0.012% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.006% MACs, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      0.223 M, 6.468% Params, 0.044 GMac, 10.205% MACs, \n",
      "      (conv1): Conv2d(0.098 M, 2.845% Params, 0.019 GMac, 4.486% MACs, 128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.002 M, 0.044% Params, 0.0 GMac, 0.070% MACs, 768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.007 M, 0.200% Params, 0.001 GMac, 0.315% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
      "      (bn2): BatchNorm2d(0.002 M, 0.044% Params, 0.0 GMac, 0.070% MACs, 768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.098 M, 2.845% Params, 0.019 GMac, 4.486% MACs, 768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.007% Params, 0.0 GMac, 0.012% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        0.017 M, 0.482% Params, 0.003 GMac, 0.765% MACs, \n",
      "        (0): Conv2d(0.016 M, 0.474% Params, 0.003 GMac, 0.748% MACs, 128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(0.0 M, 0.007% Params, 0.0 GMac, 0.012% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.006% MACs, inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv2): Conv2d(0.033 M, 0.948% Params, 0.006 GMac, 1.495% MACs, 128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(0.001 M, 0.015% Params, 0.0 GMac, 0.023% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear): Linear(2.717 M, 78.628% Params, 0.003 GMac, 0.633% MACs, in_features=256, out_features=10572, bias=True)\n",
      ")\n",
      "Computational complexity:       0.43 GMac\n",
      "Number of parameters:           3.46 M  \n"
     ]
    }
   ],
   "source": [
    "# Multiply-Accumulate\n",
    "\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "with torch.cuda.device(0):\n",
    "    macs, params = get_model_complexity_info(net, (3, 112, 112), as_strings=True,\n",
    "                                           print_per_layer_stat=True, verbose=True)\n",
    "    print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "    print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ Train epoch: 0 ]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid 3469) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-309d689eab4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0madjust_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nTime elapsed:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-63b2c5bbc0a5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    136\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    137\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Python can still get and update the process status successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0m_error_if_any_worker_fails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprevious_handler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 3469) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit."
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(0, 100):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    print('\\nTime elapsed:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_l = pd.DataFrame(train_losses, columns = ['train_losses'])\n",
    "df_train_l.plot(color = \"#ff0000\")\n",
    "plt.plot(df_train_l, marker = '*', color = 'r')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_a = pd.DataFrame(train_accuracy, columns = ['train_accuracy'])\n",
    "df_train_a.plot(color = \"#ff1111\")\n",
    "plt.plot(df_train_a, marker = '.', color = 'b')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_l = pd.DataFrame(test_losses, columns = ['test_losses'])\n",
    "df_test_l.plot(color = \"#ff0000\")\n",
    "plt.plot(df_test_l, marker = '*', color = 'r')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_a = pd.DataFrame(test_accuracy, columns = ['test_accuracy'])\n",
    "df_test_a.plot(color = \"#ff0000\")\n",
    "plt.plot(df_test_a, marker = '*', color = 'r')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
