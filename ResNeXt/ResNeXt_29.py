# -*- coding: utf-8 -*-
"""ResNeXt_29

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1exyQGfRB0K_p7_EKmpVIMnQoeDX2WU10
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter

# gpu
if torch.cuda.is_available() :
  device = torch.device('cuda')
else : 
  print("error")

# dataset
transform = transforms.Compose([
                                transforms.RandomCrop(32, padding=4),
                                transforms.RandomHorizontalFlip(),
                                transforms.ToTensor(),
                                transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))
])

train_dataset = torchvision.datasets.CIFAR10(root='./data',
                                             train = True,
                                             download =True,
                                             transform=transform)
test_dataset = torchvision.datasets.CIFAR10(root='./data',
                                            train=False,
                                            download=True,
                                            transform = transform)
train_loader = torch.utils.data.DataLoader(train_dataset,
                                           batch_size=128,
                                           shuffle=True,
                                           num_workers=4)
test_loader = torch.utils.data.DataLoader(test_dataset,
                                          batch_size=128,
                                          shuffle=False,
                                          num_workers=4)

class ResNeXt(nn.Module) :
  def __init__(self,cardinality,bottleneck_width, num_classes=10) :
    super(ResNeXt, self).__init__()

    group_width = cardinality * bottleneck_width

    self.conv1 = nn.Conv2d(3,64,kernel_size=3,stride=1,padding=1,bias=False)
    self.bn1 = nn.BatchNorm2d(64)
   
    # block 1 
    self.conv2 = nn.Conv2d(64,group_width,kernel_size=1,stride=1,padding=0,bias=False)
    self.bn2 = nn.BatchNorm2d(group_width)
    self.conv3 = nn.Conv2d(group_width,group_width,kernel_size=3,stride=1,padding=1,groups=cardinality,bias=False)
    self.bn3 = nn.BatchNorm2d(group_width)
    self.conv4 = nn.Conv2d(group_width,group_width*2,kernel_size=1,stride=1,padding=0,bias=False)
    self.bn4 = nn.BatchNorm2d(group_width*2)

    self.shortcut1 = nn.Sequential(
        nn.Conv2d(64, group_width*2, kernel_size=1, stride=1, bias=False),
        nn.BatchNorm2d(group_width*2)
        )
    # block 2 
    self.conv5 = nn.Conv2d(group_width*2,group_width,kernel_size=1,stride=1,padding=0,bias=False)
    self.bn5 = nn.BatchNorm2d(group_width)
    self.conv6 = nn.Conv2d(group_width,group_width,kernel_size=3,stride=1,padding=1,groups=cardinality,bias=False)
    self.bn6 = nn.BatchNorm2d(group_width)
    self.conv7 = nn.Conv2d(group_width,group_width*2,kernel_size=1,stride=1,padding=0,bias=False)
    self.bn7 = nn.BatchNorm2d(group_width*2)
    
    self.shortcut2 = nn.Sequential()

    # block 3 
    self.conv8 = nn.Conv2d(group_width*2,group_width,kernel_size=1,stride=1,padding=0,bias=False)
    self.bn8 = nn.BatchNorm2d(group_width)
    self.conv9 = nn.Conv2d(group_width,group_width,kernel_size=3,stride=1,padding=1,groups=cardinality,bias=False)
    self.bn9 = nn.BatchNorm2d(group_width)
    self.conv10 = nn.Conv2d(group_width,group_width*2,kernel_size=1,stride=1,padding=0,bias=False)
    self.bn10 = nn.BatchNorm2d(group_width*2)
    
    self.shortcut3 = nn.Sequential()

    group_width *= 2
    # block 4 
    self.conv11 = nn.Conv2d(group_width,group_width,kernel_size=1,stride=1,padding=0,bias=False)
    self.bn11 = nn.BatchNorm2d(group_width)
    self.conv12 = nn.Conv2d(group_width,group_width,kernel_size=3,stride=2,padding=1,groups=cardinality,bias=False)
    self.bn12 = nn.BatchNorm2d(group_width)
    self.conv13 = nn.Conv2d(group_width,group_width*2,kernel_size=1,stride=1,padding=0,bias=False)
    self.bn13 = nn.BatchNorm2d(group_width*2)
    
    self.shortcut4 = nn.Sequential(
        nn.Conv2d(group_width, group_width*2, kernel_size=1, stride=2, bias=False),
        nn.BatchNorm2d(group_width*2)
        )
    
    # block 5 
    self.conv14 = nn.Conv2d(group_width*2,group_width,kernel_size=1,stride=1,padding=0,bias=False)
    self.bn14 = nn.BatchNorm2d(group_width)
    self.conv15 = nn.Conv2d(group_width,group_width,kernel_size=3,stride=1,padding=1,groups=cardinality,bias=False)
    self.bn15 = nn.BatchNorm2d(group_width)
    self.conv16 = nn.Conv2d(group_width,group_width*2,kernel_size=1,stride=1,padding=0,bias=False)
    self.bn16 = nn.BatchNorm2d(group_width*2)

    self.shortcut5 = nn.Sequential()

    # block 6 
    self.conv17 = nn.Conv2d(group_width*2,group_width,kernel_size=1,stride=1,padding=0,bias=False)
    self.bn17 = nn.BatchNorm2d(group_width)
    self.conv18 = nn.Conv2d(group_width,group_width,kernel_size=3,stride=1,padding=1,groups=cardinality,bias=False)
    self.bn18 = nn.BatchNorm2d(group_width)
    self.conv19 = nn.Conv2d(group_width,group_width*2,kernel_size=1,stride=1,padding=0,bias=False)
    self.bn19 = nn.BatchNorm2d(group_width*2)

    self.shortcut6 = nn.Sequential()

    group_width *= 2
    # block 7 
    self.conv20 = nn.Conv2d(group_width,group_width,kernel_size=1,stride=1,padding=0,bias=False)
    self.bn20 = nn.BatchNorm2d(group_width)
    self.conv21 = nn.Conv2d(group_width,group_width,kernel_size=3,stride=2,padding=1,groups=cardinality,bias=False)
    self.bn21 = nn.BatchNorm2d(group_width)
    self.conv22 = nn.Conv2d(group_width,group_width*2,kernel_size=1,stride=1,padding=0,bias=False)
    self.bn22 = nn.BatchNorm2d(group_width*2)
    
    self.shortcut7 = nn.Sequential(
        nn.Conv2d(group_width, group_width*2, kernel_size=1, stride=2, bias=False),
        nn.BatchNorm2d(group_width*2)
        )
    
    # block 8 
    self.conv23 = nn.Conv2d(group_width*2,group_width,kernel_size=1,stride=1,padding=0,bias=False)
    self.bn23 = nn.BatchNorm2d(group_width)
    self.conv24 = nn.Conv2d(group_width,group_width,kernel_size=3,stride=1,padding=1,groups=cardinality,bias=False)
    self.bn24 = nn.BatchNorm2d(group_width)
    self.conv25 = nn.Conv2d(group_width,group_width*2,kernel_size=1,stride=1,padding=0,bias=False)
    self.bn25 = nn.BatchNorm2d(group_width*2)

    self.shortcut8 = nn.Sequential()

    # block 9 
    self.conv26 = nn.Conv2d(group_width*2,group_width,kernel_size=1,stride=1,padding=0,bias=False)
    self.bn26 = nn.BatchNorm2d(group_width)
    self.conv27 = nn.Conv2d(group_width,group_width,kernel_size=3,stride=1,padding=1,groups=cardinality,bias=False)
    self.bn27 = nn.BatchNorm2d(group_width)
    self.conv28 = nn.Conv2d(group_width,group_width*2,kernel_size=1,stride=1,padding=0,bias=False)
    self.bn28 = nn.BatchNorm2d(group_width*2)

    self.shortcut9 = nn.Sequential()


    self.linear = nn.Linear(group_width*2, num_classes)  

    
  def forward(self,x) :
    out = F.relu(self.bn1(self.conv1(x)))
    # block 1
    input = out
    out = F.relu(self.bn2(self.conv2(out)))
    out = F.relu(self.bn3(self.conv3(out)))
    out = self.bn4(self.conv4(out))
    out += self.shortcut1(input)
    out = F.relu(out)      
    # block 2
    input = out
    out = F.relu(self.bn5(self.conv5(out)))
    out = F.relu(self.bn6(self.conv6(out)))
    out = self.bn7(self.conv7(out))
    out += self.shortcut2(input)
    out = F.relu(out)   
    # block 3
    input = out
    out = F.relu(self.bn8(self.conv8(out)))
    out = F.relu(self.bn9(self.conv9(out)))
    out = self.bn10(self.conv10(out))
    out += self.shortcut3(input)
    out = F.relu(out)  
    # block 4
    input = out
    out = F.relu(self.bn11(self.conv11(out)))
    out = F.relu(self.bn12(self.conv12(out)))
    out = self.bn13(self.conv13(out))
    out += self.shortcut4(input)
    out = F.relu(out)  
    # block 5
    input = out
    out = F.relu(self.bn14(self.conv14(out)))
    out = F.relu(self.bn15(self.conv15(out)))
    out = self.bn16(self.conv16(out))
    out += self.shortcut5(input)
    out = F.relu(out)
    # block 6
    input = out
    out = F.relu(self.bn17(self.conv17(out)))
    out = F.relu(self.bn18(self.conv18(out)))
    out = self.bn19(self.conv19(out))
    out += self.shortcut6(input)
    out = F.relu(out)    
    # block 7
    input = out
    out = F.relu(self.bn20(self.conv20(out)))
    out = F.relu(self.bn21(self.conv21(out)))
    out = self.bn22(self.conv22(out))
    out += self.shortcut7(input)
    out = F.relu(out)    
    # block 8
    input = out
    out = F.relu(self.bn23(self.conv23(out)))
    out = F.relu(self.bn24(self.conv24(out)))
    out = self.bn25(self.conv25(out))
    out += self.shortcut8(input)
    out = F.relu(out)    
    # block 9
    input = out
    out = F.relu(self.bn26(self.conv26(out)))
    out = F.relu(self.bn27(self.conv27(out)))
    out = self.bn28(self.conv28(out))
    out += self.shortcut9(input)
    out = F.relu(out)          
    
    out = F.avg_pool2d(out, 8)
    out = out.view(out.size(0), -1)
    out = self.linear(out)
    return out

# increasing cardinality
def ResNeXt29_1x64d():
    return ResNeXt(cardinality=1, bottleneck_width=64)

def ResNeXt29_2x64d():
    return ResNeXt(cardinality=2, bottleneck_width=64)

def ResNeXt29_4x64d():
    return ResNeXt(cardinality=3, bottleneck_width=64)

# increasing bottleneck_width
def ResNeXt29_1x64d():
    return ResNeXt(cardinality=1, bottleneck_width=64)

def ResNeXt29_1x128d():
    return ResNeXt(cardinality=1, bottleneck_width=128)

def ResNeXt29_1x192d():
    return ResNeXt(cardinality=1, bottleneck_width=192)

# network
net = ResNeXt29_1x64d()
net = net.to(device)

# criterion,optimizer
learning_rate = 0.1
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)

# scheduler
def adjust_learning_rate(optimizer, epoch):
    lr = learning_rate
    if epoch >= 80:
        lr /= 10
    if epoch >= 120:
        lr /= 10
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr

# train, test function
def train(epoch):
    print('\n[ Train epoch: %d ]' % epoch)
    net.train()
    train_loss = 0
    correct = 0
    total = 0
    for batch_idx, (images, labels) in enumerate(train_loader):
        images = images.to(device)
        labels = labels.to(device)
        optimizer.zero_grad()

        outputs = net(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, prediction = outputs.max(1)

        total += labels.size(0)
        correct += prediction.eq(labels).sum().item()

        if batch_idx % 100 == 0:
            print('\nCurrent batch:', str(batch_idx))
            print('Current train accuracy:', str(prediction.eq(labels).sum().item() / labels.size(0)))
            print('Current train average loss:', loss.item() / 100)
            writer.add_scalar('training loss', loss.item() / 100,batch_idx)

    print('\nTrain accuarcy:', 100. * correct / total)
    print('Train average loss:', train_loss / total)
    writer.add_scalar('Accuracy on training', (100 * correct / total),epoch)

def test(epoch):
    print('\n[ Test epoch: %d ]' % epoch)
    net.eval()
    loss = 0
    correct = 0
    total = 0

    for batch_idx, (images, labels) in enumerate(test_loader):
        images = images.to(device)
        labels = labels.to(device)
        total += labels.size(0)

        outputs = net(images)
        loss += criterion(outputs, labels).item()

        _, prediction = outputs.max(1)
        correct += prediction.eq(labels).sum().item()

    print('\nTest accuarcy:', 100. * correct / total)
    print('Test average loss:', loss / total)
    writer.add_scalar('Accuracy on testing', (100 * correct / total),epoch)

    state = {
        'net': net.state_dict()
    }

    file_name = 'ResNeXt29_1x64d.pt'
    if not os.path.isdir('checkpoint'):
        os.mkdir('checkpoint')
    torch.save(state, './checkpoint/' + file_name)
    print('Model Saved')

writer = SummaryWriter()

for epoch in range(0, 300):
    adjust_learning_rate(optimizer, epoch)
    train(epoch)
    test(epoch)

writer.close()

